{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elbo-notes-miscellany.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXjqvI5qLsM7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EM + General\n",
        "\n",
        "ELBO derivation and related identities, for any density $q_n$.\n",
        "\n",
        "\\begin{align}\n",
        "\\log p(\\theta, \\mathbf{X})\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n \\log p(x_n|\\theta) & (1)\\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n \\log E_{q_n(z_n)}[p(x_n|z_n, \\theta)] & (2) \\\\\n",
        "&\\geq \n",
        "\\log p(\\theta) + \\sum_n E_{q_n(z_n)}[\\log p(x_n|z_n, \\theta)] & (3) \\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n E_{q_n(z_n)}\\bigg[\\log \\frac{p(x_n, z_n|\\theta)}{q_n(z_n)}\\bigg] & (4) \\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n E_{q_n(z_n)}[\\log p(x_n, z_n| \\theta) - \\log q_n(z_n)] & (5)\\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n E_{q_n(z_n)}[\\log p(x_n| \\theta) + \\log p(z_n | x_n, \\theta) - \\log q_n(z_n)] & (6)\\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n  E_{q_n(z_n)}[\\log p(x_n| \\theta)] + E_{q_n(z_n)}\\log p(z_n | x_n, \\theta) - \\log q_n(z_n)]  & (7)\\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n \\log p(x_n| \\theta) - D_{KL}(q_n(z_n) || p(z_n | x_n, \\theta)) & (8)\\\\\n",
        "&=\n",
        "\\log p(\\theta, \\mathbf{X}) - D_{KL}(q_n(z_n) || p(z_n | x_n, \\theta)) & (9)\\\\\n",
        "&= \n",
        "\\underbrace{\\mathcal{L}(\\theta, q)}_{``ELBO''} & (10)\n",
        "\\end{align}\n",
        "\n",
        "Notes: \n",
        "\n",
        "- EM is coordinate ascent on the ELBO.\n",
        "\n",
        "- E-step: maximize ELBO w.r.t. $q$ (the hidden/approximating thing)\n",
        "\n",
        "- M-step: maximize ELBO w.r.t. $\\theta$ (model params). \n",
        "\n",
        "- (5) is a common starting point\n",
        "\n",
        "- Look at (8) hard enough and you see the objective for amortized VI.\n",
        "\n",
        "- When $q_n = p(z_n | x_n, \\theta)$, the ELBO is tight and the \"$\\geq$\" in line 3 becomes a \"$=$\". Proof:\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\theta, q)\n",
        "& = \n",
        "\\log p(\\theta) + \\sum_n E_{q_n(z_n)}[\\log p(x_n, z_n| \\theta) - \\log q_n(z_n)]\\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n E_{p(z_n | x_n, \\theta)}[\\log p(x_n, z_n| \\theta) - p(z_n | x_n, \\theta)]\\\\\n",
        "&=\n",
        "\\log p(\\theta) + \\sum_n E_{p(z_n | x_n, \\theta)}\\bigg[\\log \\frac{p(x_n, z_n | \\theta)}{p(z_n|x_n, \\theta)}\\bigg] \\\\\n",
        "&=\n",
        "\\log p(\\theta) \\sum_n E_{p(z_n | x_n, \\theta)}\\log p(x_n | z_n, \\theta) \\\\\n",
        "&=\n",
        "\\log p(\\theta) \\sum_n \\log p(x_n | \\theta) \\\\\n",
        "&=\n",
        "\\log p(\\theta, \\mathbf{X}).\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "MuAqpjFukr9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean-Field Approximation\n",
        "\n",
        "First, helpful to remember\n",
        "$$\n",
        "E_{q_{1:m}}[\\log q(z_{1:m})] = \\sum_{i}^m E_{q_i}[\\log q(z_i)]\n",
        "$$\n",
        "\n",
        "Try to smush together:\n",
        "\n",
        "\\begin{align}\n",
        "D_{KL}(q(\\theta; \\lambda)||p(\\theta|x))\n",
        "&=\n",
        "E_{q(\\theta; \\lambda)}[\\log q(\\theta; \\lambda)] - E_{q(\\theta; \\lambda)}[\\log p(\\theta | \\mathbf{X})] \\\\\n",
        "&=\n",
        "E_{q(\\theta; \\lambda)}[\\log q(\\theta; \\lambda)] - E_{q(\\theta; \\lambda)}[\\log p(\\theta , \\mathbf{X}) - \\log p(\\mathbf{X})]  \\\\\n",
        "&=\n",
        "E_{q(\\theta; \\lambda)}[\\log q(\\theta; \\lambda)] - E_{q(\\theta; \\lambda)}[\\log p(\\theta , \\mathbf{X})] + E_{q(\\theta; \\lambda)}[\\log p(\\mathbf{X})]  \\\\\n",
        "&=\n",
        "\\underbrace{E_{q(\\theta; \\lambda)}[\\log q(\\theta; \\lambda)] - E_{q(\\theta; \\lambda)}[\\log p(\\theta , \\mathbf{X})]}_{-ELBO(\\lambda) := \\mathcal{L}(\\lambda)} + \\log p(\\mathbf{X}).\n",
        "\\end{align}\n",
        "\n",
        "Hence, zooming in on $\\mathcal{L}(\\lambda)$, we have by the mean-field assumptions, we have:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\lambda)\n",
        "&=\n",
        "E_{q(\\theta; \\lambda)}[\\log p(\\theta , \\mathbf{X})] - \n",
        "E_{q(\\theta; \\lambda)}[\\log q(\\theta; \\lambda)] \\\\\n",
        "&=\n",
        "E_{\\prod_{i=1}^N q(\\theta_i; \\lambda_i)}[\\log p(\\theta , \\mathbf{X})] - \n",
        "E_{\\prod_{i=1}^N q(\\theta_i; \\lambda_i)}[\\log q(\\theta; \\lambda)] \\\\\n",
        "&=\n",
        "E_{\\prod_{i=1}^N q(\\theta_i; \\lambda_i)}[p(\\theta, \\mathbf{X})] - \n",
        "E_{\\prod_{i=1}^N q(\\theta_i; \\lambda_i)}\\bigg[\\log \\prod_{i=1}^N q(\\theta_i; \\lambda_i)\\bigg] \\\\\n",
        "&=\n",
        "E_{\\prod_{i=1}^N q(\\theta_i; \\lambda_i)}[\\log p(\\theta , \\mathbf{X})] \n",
        "- \n",
        "E_{\\prod_{i=1}^N q(\\theta_i; \\lambda_i)}\\bigg[\\sum_{i=1}^N \\log q(\\theta_i; \\lambda_i)\\bigg]\n",
        "\\end{align}\n",
        "\n",
        "Then, for a particular $j$, we can look at\n",
        "\n",
        "\\begin{align}\n",
        "...&=\n",
        "E_{q(\\theta_j; \\lambda_j) \\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta , \\mathbf{X})] - \n",
        "E_{q(\\theta_j; \\lambda_j) \\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}\\bigg[\\sum_{i=1}^N \\log q(\\theta_i; \\lambda_i)\\bigg]\\\\\n",
        "&=\n",
        "E_{q(\\theta_j; \\lambda_j) \\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta_j |\\theta_{i \\neq j}, \\mathbf{X}) + \\log p(\\theta_{i \\neq j}, \\mathbf{X})] - \\\\&\\hspace{1.5cm}\n",
        "E_{q(\\theta_j; \\lambda_j) \\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}\\bigg[\\sum_{i=1}^N \\log q(\\theta_i; \\lambda_i)\\bigg]\\\\\n",
        "&=\n",
        "E_{q(\\theta_j; \\lambda_j) \\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta_j |\\theta_{i \\neq j}, \\mathbf{X}) + \\log p(\\theta_{i \\neq j}, \\mathbf{X})] - \\\\&\\hspace{1.5cm}\n",
        "E_{q(\\theta_j; \\lambda_j)}[\\log q(\\theta_j; \\lambda_j)] +  E_{\\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}\\bigg[\\sum_{i\\neq j} \\log q(\\theta_i; \\lambda_i)\\bigg]\\\\\n",
        "&=\n",
        "E_{q(\\theta_j; \\lambda_j) \\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta_j |\\theta_{i \\neq j}, \\mathbf{X}) + \\log p(\\theta_{i \\neq j}, \\mathbf{X})] - \n",
        "E_{q(\\theta_j; \\lambda_j)}[\\log q(\\theta_j; \\lambda_j)] +  c\\\\\n",
        "&=\n",
        "E_{q(\\theta_j; \\lambda_j)}E_{\\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta_j |\\theta_{i \\neq j}, \\mathbf{X}) + \\log p(\\theta_{i \\neq j}, \\mathbf{X})] - \n",
        "E_{q(\\theta_j; \\lambda_j)}[\\log q(\\theta_j; \\lambda_j)] +  c\\\\\n",
        "&=\n",
        "E_{q(\\theta_j; \\lambda_j)}[E_{\\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta_j |\\theta_{i \\neq j}, \\mathbf{X})]] - \n",
        "E_{q(\\theta_j; \\lambda_j)}[\\log q(\\theta_j; \\lambda_j)] +  c'.\\\\\n",
        "\\end{align}\n",
        "\n",
        "Note that the mean-field independence assumption allows us to separate/stack up the expectations as we did. Then, if we call (up to additive constant)\n",
        "\n",
        "$$\n",
        "\\log \\tilde p(\\theta_j) = E_{\\prod_{i\\neq j} q(\\theta_i; \\lambda_i)}[\\log p(\\theta_j |\\theta_{i \\neq j}, \\mathbf{X})],\n",
        "$$\n",
        "\n",
        "the above algebra simplifies to \n",
        "\n",
        "$$\n",
        "-D_{KL}(q(\\theta_j; \\lambda_j) || \\tilde p(\\theta_j)) + c'.\n",
        "$$\n",
        "\n",
        "So to minimize KL for a particular $\\theta_j, \\lambda_j$ pair, you choose\n",
        "\n",
        "$$\n",
        "q(\\theta_j; \\lambda_j) = \\tilde p(\\theta_j).\n",
        "$$\n",
        "\n",
        "Hence a CAVI step means setting equal to the full conditional, analagous to a Gibbs sampler.\n"
      ],
      "metadata": {
        "id": "OJWXNN5JY-k3"
      }
    }
  ]
}