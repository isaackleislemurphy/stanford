---
title: "HW4"
author: "Isaac Kleisle-Murphy"
date: "3/4/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(survival)
library(KMsurv)
library(glmnet)
library(ggfortify)
library(MASS)
library(tidyr)

data(burn)
```


## 11.3
### a.)
```{r}
data = matrix(
  c(955, 162, 9, 188),
  byrow = TRUE, ncol = 2
) %>%
  data.frame() %>%
  `colnames<-`(c("yes_hell", "no_hell")) %>%
  `rownames<-`(c("yes_heav", "no_heav"))

data
```
As set forth in Agresti 11.1, we compute statistic (using marginal proportions) $d=\hat p_{\cdot, 1} - \hat p_{1, \cdot}$, as well as (take the corners) $\hat \sigma^2_{d} = \frac{\hat p_{1,2} - \hat p_{2,1} - (\hat p_{1,2} + \hat p_{2,1})^2}{\sum p_{ij}}$, and then we get a Wald CI with $d$ as mean and $\hat \sigma_d^2$ as variance. Coded up, this is:
```{r}
p1_ = (rowSums(data) / sum(data))[1]
p_1 = (colSums(data) / sum(data))[1]
d = p1_ - p_1

p12 = data[1, 2] / sum(data); p21 = data[2, 1] / sum(data)
sigma2_d = (p12 + p21 - (p12 - p21)^2) / sum(data)

cat("d =", d, "\nsigma2 =", sigma2_d)
```
And hence a Wald CI of:
```{r}
d - qnorm(.975) * sqrt(sigma2_d); d + qnorm(.975) * sqrt(sigma2_d)
```

We then say that we are 95% confident that the true difference in marginal proportions between those who believe in heaven and those who believe in hell is between .098 and .135. 

### b.)

As detailed in Agresti 11.1.2, our test statistic under null is $T = \frac{d}{\hat\sigma^{(d)}_0} = \frac{n_{21} - n_{12}}{(n_{21} + n_{12})^{1/2}}$, where our null is that the marginals are equal ($H0: d=0 \iff \pi_{1, \cdot} = \pi_{\cdot, 1}$):
```{r}
(162 - 9) / sqrt(162 + 9)
```
We then square this statistic, and test $T^2$ against a $\chi^2_{1}$. The p-value is zero, leading to a rejection:
```{r}
pchisq(11.7 * 11.7, 1, lower.tail=F)
```

Hence, we reject the marginal homogeneity assumption -- clearly there is a significant difference in the marginal proportions, for a "yes" answer belief in belief in heaven and belief in hell. 

### c.)
As set forth in Agresti 11.1, we have that
$$
VAR(\sqrt n d) 
= 
\pi_{1, \cdot}(1 - \pi_{1, \cdot}) + \pi_{\cdot, 1}(1 - \pi_{\cdot, 1}) - 2 (\prod_{i=j}\pi_{ij} - \prod_{i\neq j}\pi_{ij})
=
VAR(\pi_{1, \cdot}) + VAR(\pi_{\cdot, 1}) - 2COV(\pi_{1, \cdot}, \pi_{\cdot, 1}) 
$$
And if the covariance term is nonzero and large, we know there is dependence between the two. Hence, when we see this massive value:
```{r}
data[2, 2] * data[1, 1] - data[1, 2] * data[2, 1]
```
we can be pretty confident in a high covariance, and hence dependence. 

Now, had we taken independent samples, then we'd have $2COV(\pi_{1, \cdot}, \pi_{\cdot, 1})  = 0$, due to the independence of our procedure. Accordingly, this covariance would not be baked into our variance calculation, and any corresponding test would overlook the fundamental dependence here. This could induce a misleading test result.

## 11.9


### a.)
Recall the symmetric setup, i.e.
$$
\log(\mu_{ab}) = \lambda + \lambda_{a} + \lambda_b + \underbrace{\lambda_{ab}}_{=\lambda_{ba}}
$$
We then wrangle:
```{r}
data_base = data.frame(first_purchase=c("high_point", "tasters_choice", "sanka", "nescafe", "brim"),
  high_point=c(93, 9, 17, 6, 10),
  tasters_choice=c(17, 46, 11, 4, 4),
  sanka=c(44, 11, 155, 9, 12),
  nescafe=c(7, 0, 9, 15, 2),
  brim=c(10, 9, 12, 2, 27)
)

data =  data_base %>%
  pivot_longer(
    high_point:brim, names_to="second_purchase", values_to="count", 
  ) %>%
  mutate(
    sym_pairing = ifelse(
      first_purchase < second_purchase,
      paste0(first_purchase, "-",  second_purchase),
      paste0(second_purchase, "-",  first_purchase)
    )
  )
data
```
and fit the symmetry model:
```{r}
glm(count ~ sym_pairing, data, family=poisson()) %>%
  summary()
```

We see a residual deviance from an empty Poisson regression of 22.47; on 10 df, this is concerning, as we'd want residual deviance roughly equal to degrees of freedom. Our fit here appears to be poor. If we say $H_0$ is symmetry, and test using this statistic, we get a p-value of
```{r}
1 - pchisq(22.47, 10)
```
which likely results in rejection of the nullmodel, i.e. symmetry. On inspection, it's clear that much of this rejection is powered by the Sanka/high-point pairing, which has heavily imbalanced corners (17 and 44):
```{r}
data_base
```
This `high_point` vs. `sanka` imbalance causes `(44 - 17) / sqrt(44 + 17) ~= 3.5`, or over 9 points to the $X^2$. This is the bulk of the differential between $X^2$ (analogously, $G^2$) and `df`. 

### b.) 
The marginal homogeneity test, as set forth in Agresti 11.25, compares the symmetry model with the quasi-symmetry model. So we need to fit the quasi-symmetry model, which recall takes the form
$$
\log(\mu_{ab}) = \lambda + \lambda_a + \lambda_b + \lambda_{ab},
$$
but now $\lambda_{ab} =\lambda_{ba}$ for $a < b$

Fit it:
```{r}
glm(count ~ sym_pairing + first_purchase, data=data, family=poisson()) %>%
  summary()
```
As instructed by Agresti, we subtract the residual deviance from the symmetry model from that of the quasi-symmetry, i.e. 
$$
G^2(Sym) - G^2(QuasiSym) = 22.473 - 9.974=  12.499. 
$$
The delta in degrees of freedom is 10-6=4 so we test 12.499 against 4 df:
```{r}
1 - pchisq(12.499, 4)
```
Here, our null hypothesis would have been marginal homogeneity; this p-value likely induces (for reasonable $\alpha = .05$) a rejection in favor of marginal non-homogeneity. This small p-value is again overwhelmingly driven by the High Point / Sanka pairing. If we do a one vs. all collapsing (i.e. high point vs. the field), and take a McNemar's statistic, we have
```{r}
data %>%
  mutate(
    first_purchase = ifelse(first_purchase == "high_point", "high_point", "other"),
    second_purchase = ifelse(second_purchase == "high_point", "high_point", "other")
  ) %>%
  group_by(first_purchase, second_purchase) %>%
  summarise(count_ = sum(count)) -> data_collapsed
data_collapsed
```
This gives a 2x2 table of (high point in 0 index always; row = first; column = second)
$$
\begin{bmatrix}
& HP2 & O2 \\
HP1 & 93 & 78\\
O1 & 42 & 328
\end{bmatrix}
$$
The McNemar's stat here is $(78 - 42)/\sqrt{78 + 42} = 3.286$. This value is pretty extreme, showing how  `high_point` -- in particular, the imbalance between `high_point` and `sanka` (also demonstrated above) -- is again driving the rejection. 

For the Wald CI, we use the procedure in the previous problem: compute $d$ and $\hat\sigma^2$, and back out a CI from this. That is, s set forth in Agresti 11.1, we compute statistic (using marginal proportions) $d=\hat p_{\cdot, 1} - \hat p_{1, \cdot}$, as well as (take the corners) $\hat \sigma^2_{d} = \frac{\hat p_{1,2} - \hat p_{2,1} - (\hat p_{1,2} + \hat p_{2,1})^2}{\sum p_{ij}}$, and then we get a Wald CI with $d$ as mean and $\hat \sigma_d^2$ as variance. Coded up, this is:
```{r}

data_ = matrix(c(93, 78, 42, 328), byrow=T, ncol=2)

p1_ = (rowSums(data_) / sum(data_))[1]
p_1 = (colSums(data_) / sum(data_))[1]
d = p_1 - p1_

p12 = data_[1, 2] / sum(data_); 
p21 = data_[2, 1] / sum(data_)
sigma2_d = (p12 + p21 - (p12 - p21)^2) / sum(data_)

cat("d =", d, "\nsigma2 =", sigma2_d)
```

The Wald CI is thus
```{r}
d - sqrt(sigma2_d) * qnorm(.975); d + sqrt(sigma2_d) * qnorm(.975)
```
That is, we say with about 95% confidence that the true difference in marginal proportions is between around -.105 and -.027.


### c.)
First, let's wrangle the diagonal indicators before fitting either
```{r}
data_diag = data %>%
  mutate(diag_indicator = ifelse(
    first_purchase == second_purchase,
    first_purchase,
    "0")
  )

data_diag
```
We then proceed to fit the quasi-independence model, in order to test marginal homogeity
```{r}
glm(count ~ first_purchase + second_purchase + diag_indicator, data=data_diag, family=poisson()) %>%
  summary()
```
Just as anticipated: 13.78 residual deviance on 11 df. With a p-value of
```{r}
1- pchisq(13.786, 11)
```
we'd have little reason to reject a null hypothesis of the quasi-independence model. We can live with this quasi-independence model, which implies that when people change brands, it appears their second choice is independent of their first choice.

By contrast, the *independence* model looks like
```{r}
glm(count ~ first_purchase + second_purchase , data=data_diag, family=poisson()) %>%
  summary()
```
346.8 residual deviance on 16 degrees of freedom is awful! This would be a terrible fit, taking p-value 
```{r}
1 - pchisq(346.78, 16)
```
and a clear-cut rejection of this model. 


## 11.11

```{r}
library(stringr)
#' Recycling code above
data_base = data.frame(
  father_group = c("fat1", "fat2", "fat3", "fat4", "fat5"),
  son1 = c(50, 28, 11, 14, 3),
  son2 = c(45, 174, 78, 150, 42),
  son3 = c(8, 84, 110, 185, 72),
  son4 = c(18, 154, 223, 714, 320),
  son5 = c(8, 55, 96, 447, 411)
)
data_base

data =data_base %>%
  pivot_longer(
    son1:son5, names_to="son_group", values_to="count", 
  ) %>%
  mutate(
    # for ordinal
    father_group_ord = str_replace_all(father_group, "fat", ""),
    father_group_ord = as.numeric(father_group_ord),
    son_group_ord = str_replace_all(son_group, "son", ""),
    son_group_ord = as.numeric(son_group_ord),
    # symmetry pairings
    sym_pairing = ifelse(
      father_group_ord < son_group_ord,
      paste0(father_group_ord, "-",  son_group_ord),
      paste0(son_group_ord, "-",  father_group_ord)
    ),
    # diagonal indicators
    diag_indicator = ifelse(
      father_group_ord == son_group_ord,
      as.character(father_group_ord),
      "0"
    )
  )

# symmetry
fit_a = glm(count ~ sym_pairing, data=data %>% arrange(son_group), family=poisson())
# quasi symmetry
fit_b = glm(count ~ sym_pairing + father_group, data=data, family=poisson())
# ordinal quasi symmetry
# put feature u = 1, .., 5 in for father group, instead of using the one-hot encoding. That is,
# undiscretize father group
fit_c = glm(count ~ sym_pairing + father_group_ord, 
            data=data, 
            family=poisson()
)

fit_e = glm(
  count ~ father_group + son_group + diag_indicator,
  data=data,
  family=poisson()
)

### helpers
testmod <- function(mod){1 - pchisq(mod$deviance, mod$df.residual)}
analyze <- function(mod){
  cat("Residual Deviance: ", mod$deviance, "\nResidual DF: ", mod$df.residual)
  cat("\nP-Value: ", testmod(mod))
}

```

### a.)
```{r}
analyze(fit_a)
```
With 37 deviance on 10 degrees of freedom, this appears to be a poor fit -- so poor, a hypothesis test would likely reject it. 

### b.)
```{r}
analyze(fit_b)
```
Here, residual deviance is in line with residual df; the high p-value returns a no rejection, so this is probably a decent model.

### c.)
```{r}
analyze(fit_c)
```
While not as extreme as in a.), residual deviance is well above residual df, and a p-value of .04 rejects at alpha .05. This is probably a poor fit as well. 

### d.)
Recall we test using delta between symmetric and quasi-symmetric, i.e.
```{r}
cat("S-QS Deviance: ", fit_a$deviance - fit_b$deviance, "\nS-QS DF: ", fit_a$df.residual - fit_b$df.residual)
cat("\nP-Value: ", 1 - pchisq(fit_a$deviance - fit_b$deviance, fit_a$df.residual - fit_b$df.residual))
```

As before, this too is likely to get rejected if chosen as a null model, given the small p-value here.

### e.)
```{r}
analyze(fit_e)
```
Same story as before, residual deviance way ahead of residual df. P-value is zero, so this gets rejected as a null.

Given that the quasi-independence model is the only one to not get rejected, it's probably best here. 


## 11.26
*Q: Explain the following analogy: McNemarâ€™s test is to binary data as the paired difference t test is to normally distributed data.*

First, there is an obvious duality between the two: whereas paired t-tests test the difference between two real-valued variables that are dependent on one another, McNemar tests the difference between two marginal proportions/percentages, which are necessarily dependent (in order to fill out the data table). That is, the McNemar's example boils down to the single-voter example for Bush/Gore vs. McCain/Obama: wherever the one voter lands, the rest of the cells are all zero and hence dependent on that first cell.

Moreover, it's also worth noting that generally both begin with an $H_0$ of zero difference in proportion: for McNemar, this is $\pi_{1, \cdot} - \pi_{\cdot, 1} = 0$, whereas for paired it is that $X-Y =0$. 


## 11.34
Consider the following counter example,
$$
P \propto 
\begin{bmatrix}
1 & 3 & 2 \\
2 & 1 & 3 \\
3 & 2 & 1
\end{bmatrix}
$$
Clearly, this will satisfy marginal homogeneity as for any $i, j$, we have that (recall that diagonal doesn't count, so we zero it out)
$\sum_{j\neq i} p_{i, j}= \sum_{i\neq j} p_{i, j}$. However, $p_{12}\neq p_{2,1}$, for example, so it is non symmetric. 

## 11.32

### a.)
For setup and me to keep track of, we have under Agresti's Ch. 11 specification that

- $t=1, 2$ are the timesteps.
- $s_i = y_{i, t=1} + y_{i, t=2}$ Are the observations
- $Y_{i, t}, S_i$ are the corresponding RVs
- $\beta \in \mathbb{R}^p$ are the coefficients
- $X \in \mathbb{R}^{N\times p \times t}$ is a tensor of features. 
- $x_{i, :, t} \mathbb{R}^p$ is a vector of one realization of features at time $t$. 
- The model is then given by, for the $t=1, 2$ scenario
$$
P(Y_{i, t} = 1) = Logit^{-1}( x_{i, :, t}^T\beta).
$$
In turn, when looking at the $S_i = 1$ cases, we have
\begin{align*}
P(Y_{i, 1}=0, Y_{i, 2} = 1 | S_i = 1) \propto P(Y_{i, 0}=0, Y_{i, 1} = 1 ,  S_i = 1) \\
&\propto  
\underbrace{
\bigg(
\frac{1}{1 + exp(x_{i, :, 1}^T\beta)}
\bigg)
}_{P(Y_{i, 1} = 0)}
\cdot
\underbrace{
\bigg(
\frac{exp(x_{i, :, 2}^T\beta)}{1 + exp(x_{i, :, 2}^T\beta)}
\bigg)
}_{P(Y_{i, 2} = 1)} \\
&\propto
exp(x_{i, :, 2}^T\beta) \\
=
\frac{exp(x_{i, :, 2}^T\beta)}{exp(x_{i, :, 1}^T\beta) + exp(x_{i, :, 2}^T\beta)}
\end{align*}
And identically
$$
P(Y_{i, 1}=1, Y_{i, 2} = 0 | S_i = 1) \propto exp(x_{i, :, 1}^T\beta) = \frac{exp(x_{i, :, 1}^T\beta)}{exp(x_{i, :, 1}^T\beta) + exp(x_{i, :, 2}^T\beta)}.
$$
The final step above follows from the requisite normalization under conditioning.

In these cases, the dependence on $\beta$ is clear cut. In contrast, consider what happens when $S_i = 0$. Deterministically, it must be that $Y_{i, 1} = 0, Y_{i, 2} = 0 $ under these conditions, and hence
$$
P(Y_{i, 1} = a,  Y_{i, 2}= b | S_i = 0) = \begin{cases}
1 & a = b = 0\\
0 & else
\end{cases}.
$$
Similarly, when $S_i = 2$, it has to be that $Y_{i, 1} = Y_{i, 2} = 1$ (otherwise a contradiction), giving 
$$
P(Y_{i, 1} = a,  Y_{i, 2}= b | S_i = 2) = \begin{cases}
1 & a = b = 1\\
0 & else
\end{cases}.
$$
Here, due to the deterministic nature of the cases, there is no dependence on $\beta$. 

### b.)
Next, we consider what happens to the corresponding coefficient when one of the features never changes under a timestep. Arbitrarily, say $1 \leq j \leq p$ is that feature, we now care about $\beta_j$. Now, when $S_i\in\{0, 2\}$, we know the conditional distribution won't depend on this $\hat\beta_j$; we just showed this above. However, when (using $\sim j$ to denote inclusion of all indices except $j$) $S_i = 1$, we have

\begin{align*}
P(Y_{i, 1}=1, Y_{i, 2} = 0 | S_i = 1) 
&= 
\frac{exp(x_{i, :, 1}^T\beta)}{exp(x_{i, :, 1}^T\beta) + exp(x_{i, :, 2}^T\beta)}\\
&= 
\frac{
exp(x_{i, \sim j, 1}^T\beta_{\sim j}+ x_{i, j, 1} \beta_j)
}{
exp(x_{i, \sim j, 1}^T\beta_{\sim j} + x_{i, j, 1} \beta_j) + exp(x_{i, \sim j, 2}^T\beta_{\sim j}+ x_{i, j, 2} \beta_j)
} \\
&=
\frac{
exp(x_{i, \sim j, 1}^T\beta_{\sim j})\exp(x_{i, j, 1} \beta_j)
}{
exp(x_{i, \sim j, 1}^T\beta_{\sim j})\exp(x_{i, j, 1} \beta_j) + exp(x_{i, \sim j, 2}^T\beta_{\sim j})\exp(x_{i, j, 2} \beta_j)
} \\
&=
\frac{
exp(x_{i, \sim j, 1}^T\beta_{\sim j})\exp(x_{i, j, 1} \beta_j)
}{
exp(x_{i, \sim j, 1}^T\beta_{\sim j})\exp(x_{i, j, 1} \beta_j) + exp(x_{i, \sim j, 2}^T\beta_{\sim j})\exp(x_{i, j, 1} \beta_j)
} \\
&=
\frac{
exp(x_{i, \sim j, 1}^T\beta_{\sim j})
}{
exp(x_{i, \sim j, 1}^T\beta_{\sim j}) + exp(x_{i, \sim j, 2}^T\beta_{\sim j})
},
\end{align*}
and here, there is no dependence on $\hat\beta_j$. 

## Staphyllococcus
```{r}
head(burn)
```


### 1.)
The stratified K-M plot is presented below:
```{r}
survfit(Surv(T3, D3) ~ Z1, data=burn) -> km_fit
autoplot(km_fit)
```

### 2.)
Perform the test:
```{r}
survdiff(Surv(T3, D3) ~ Z1, data=burn)
```

It's a really close call, with a p-value of .05. Assuming we're testing at $\alpha = .05$, we'll call this a rejection, and say there is some meaningful difference in time among treatment groups.

### 3.)
```{r}
coxph(Surv(T3, D3) ~ Z1, data=burn) -> cox_fit
summary(cox_fit)
```

Here, the score test is nearly identical (3.76 on 1 df, as opposed to the K-M's 3.8 on 1), with a similar p-value of $.05$. The conclusion largely remains the same.

#### 4.) 
```{r}
cox_full = coxph(
  Surv(T3, D3) ~ Z1 + Z2 + Z3 + Z4 + Z5 + Z6 + Z7 + Z8 + Z9 + Z10  + as.factor(Z11), 
  data=burn
)
summary(cox_full)
```

Conditional on all others being included in the model, we see that `Z1`, `Z3`, and to a lesser extent, `Z11` are most helpful. Note that `Z11`, having the four categories `1, 2, 3, 4`, is treated as categorical via one-hot encoding.

### 5.)
```{r}
M = coxph(
  Surv(T3, D3) ~ Z1 + Z2 + Z3 + Z4 + Z5 + Z6 + Z7 + Z8 + Z9 + Z10  + as.factor(Z11), 
  data=burn
)
D = model.frame(M) # treats diagnosis as continuous
X = model.matrix(M)
Y = D[,1]

cox_lasso = cv.glmnet(X, Y, family='cox')
plot(cox_lasso)

cox_lasso_lambda_min=glmnet(X, Y, family="cox", lambda=cox_lasso$lambda.min)
cox_lasso_c_idx=cv.glmnet(X, Y, family="cox", type.measure="C")
```
The variables under `lambda.min` are:
```{r}
coef(cox_lasso_lambda_min)
```
 As discussed above, `Z1`, `Z3` and `Z11=3` already looked like strong candidates, given their apparent conditional importance. This suggests that we add `Z2` and `Z6` to that feature pool. Note that the inclusion of only `Z11=3` suggests we should consider binarizing this categorical variable. 
 
 
 ### 6.)
 Meanwhile, under `C-Index`, we have
```{r}
coef(cox_lasso_c_idx)
```
 
 Here, we only select `Z1`, `Z3` and `Z11=3`.
 
 ### 7.)
 If we want to take an all-or-none approach to the three `Z11` indicators (which combined to make a one-hot-encoded `Z11`), we might turn to group lasso, and minimize w.r.t.
 $$
 \ell(X\beta) + \lambda_1 ||\beta_{Z\neq 11}||_1 +  \lambda_2 ||\beta_{Z = 11}||_2^{1/2}
 $$
## Bone Marrow

### 1.) 
Look at the data:
```{r}
data(bmt)
head(bmt)
```
Now fit the K-M curves:
```{r}
survfit(Surv(t2, d3) ~ group, data=bmt) -> km_fit
autoplot(km_fit)
```

### 2.)
Test results are shown at the bottom of the summary:
```{r}
survdiff(Surv(t2, d3) ~ group, data=bmt)
```

 A p-value of .001 indicates a clear rejection, and there is likely some difference in survival among groups.
 
 As set forth in the Survival I slides, we have, for any time point $j$:
 
 - rows (the two dimensional -- using the HW framing here, as the slides transpose the table to 3x2) correspond to (i) counts of deaths up to time $T_j$ and (2) counts of survivals past time $T_j$. In other words, rows loosely represent a "check-in" at time $j$, where we tally up deaths and survivals at that point. 
 
 - columns (the three dimensional) correspond to `group`. 
 
 - We do this over every sampled time point $j$, so $K$ equals the number of unique timestamps (across all groups) we collected.
 
Whereas the hypergeometric distribution falls nicely out of the 2x2 case, here we generalize to a multivariate hypergeometric (as it's a 3x2, or perhaps larger table), wherein the number of deaths/non-survivals within a group (i.e. failures) corresponds to a three-binned urn. That is, instead of counting ways we could put marbles into one bin or the other, we're divvying up how they go into three bins (see: https://en.wikipedia.org/wiki/Hypergeometric_distribution#Multivariate_hypergeometric_distribution)
 
### 3.)
 
 As detailed in the `survival::survdiff` documentation, the `exp` attribute provides the expected number of events (deaths) within each group. That is, you fit the model on each observation. Then, for each time $T_j$, you compute the expected number of deaths up to that point via (for group $k$)
 $$
 \delta_{k_j, j}\frac{|R_k(T_j)}{|R(T_j)|}
 $$
 You then take the last of these (i.e. how long your survivors were allowed to survive, the survival opportunity window) -- this is your expected number of deaths in the time period for the group. You can compare that to the expected. 
 
In a similar spirit, the `var` attribute just gives the variance between the aforementioned counts -- for example, with three groups, there'll be three expected counts, and hence a $3\times 3$ covariance. 
The diagonal of these are variances.

Now, for your survival opportunity window, you have observed counts, expected counts (above), and variances of these expected counts (from the covariance). This lays the groundwork for the chi-squared test, i.e. for each group $k$ computing $(O_k - E_k / V_k)^2$, summing it up, and using that as your test statistic. The test statistic itself was derived in class. 

### 4.)



```{r}
empty_fit = coxph(Surv(ta, da) ~ 1, data=bmt)
full_fit = coxph(Surv(ta, da) ~ z10 * as.factor(group), data=bmt)
step_fit = step(empty_fit, scope=formula(full_fit), direction="forward", trace=2)
# step_fit = stepAIC(empty_fit, scope=formula(full_fit), direction="forward", trace=2)
step_fit$anova
```

Unfortunately, the interaction is not selected -- in fact no model is selected. However, when we begin a stepwise regression with `~ z10 * group`, and consider the other covariates (`z1` - `z10`), we get (allowing all the z's to interact with group)
```{r}
actually_full_fit = coxph(Surv(ta, da) ~ (
  z1 + z2 + z3 + z4 + z5 + z6 + z7 + z8 + z9 + z10) * as.factor(group), 
  data=bmt
)
step_fit_2 = step(full_fit, scope=formula(actually_full_fit), direction="forward", trace=2)
step_fit_2$anova

final_fit = coxph(formula(step_fit_2), data=bmt)
```
Here, the stepwise regression adds in `z1` and `z6` (consistent with their important utility noted above). 

```{r}
summary(final_fit)
```

As shown above, the main effect for `MTX=z10` on its own takes coefficient $-0.55326$. The CI for this coefficient is
```{r}
-.5536 - 1.96 * 0.70815; -.5536 + 1.96 * 0.70815
```
Case 1: `group = 1`. 

Exponentiated, this amounts to $.5751$ with CI ($.14353, .17389$). In other words, this means pretty squarely that when `z10` is 1, the hazard decreases for `group = 1`, the baseline group, which is *good* for the survival of the patient. 

Case 2: `group = 2`.

Then, when `group = 2`, we have linear contribution $\beta_{z10} + \beta_{z10, G2} = 0.69359  -0.55326 = -1.24685$. As $\beta_{z10, G2} < 0$, this makes $\beta_{z10} + \beta_{z10, G2} < \beta_{z10}$, and makes the hazard even smaller. So with `group=2`, the protection is even stronger (CI omitted since covariance of coefficients not provided, which is necessary for the CI). 

Case 3: `group = 3`.

Lastly, when `group = 3`, we have linear contribution $\beta_{z10} + \beta_{z10, G3} = 1.2043  -0.55326 = .65104$. This increases the linear contribution, in turn increasing the hazard, and predicting that the patient is less likely to survive (as their hazard has increase) as compared to a group 1 or 2 patient. 

Of course, there's little confidence to be had here. All of the confidence intervals, upon exponentiation, contain 1, so we can't say with much force that hazard is increasing or decreasing. A null of no change in hazard would fail to be rejected. 




## Conditional Independence

### a.)
Leaning conditional independence here, as features may have relationship with data availability, and in turn a relationship with censoring? Consider the following (bad) example: let's say you're doing a survival study of football players, and the survival event equals first concussion and time is measured in snaps taken. Here, weight might reasonably be a feature in your model -- if you're bigger, maybe you're more likely to be the hammer and not the nail. However, at the same time, players at lower weights may see the field less. As a result, these players might play fewer snaps, and by the end of the season, they'd hardly have had a chance to get concussed. In this way, the feature `X` would be related to `D` -- suggesting that conditional independence may be well-suited in certain cases. 





### b.)
Consider the $\delta=1$ case first. We have, using $M, m$ instead of $O, o$ for notational convenience
\begin{align*}
P_\beta(M\in[m, m + dm], \delta = 1|X)
&=
P_\beta(T\in [m, m+dm], C > T | X) \\
&=
\int_{v=0}^\infty\int_{u=0}^\infty f^{(\beta)}_{C, T|X}(u, v)\delta(v\in [m, m+dm])\delta(u > v)du duv\\
&=
\int_{v=m}^{m+dm}\int_{u=v}^\infty f^{(\beta)}_{C, T|X}(u,v) du dv
\end{align*}

Now, if we have conditional independence, we may separate $f_{C, T|X}$ and continue with the proof. Otherwise, as is the case for independence (which does not imply cond. independence), we are stuck here, unless $f_{C, T|X}$ is an especially nice density (unlikely) to integrate over. Now, proceeding forward \textit{with} conditional independence, we can then do

\begin{align*}
P_\beta(M\in[m, m + dm], \delta = 1|X)
&=
P_\beta(T\in [m, m+dm], C > T | X) \\
&=
\int_{v=0}^\infty\int_{u=0}^\infty f^{(\beta)}_{C, T|X}(u, v)\delta(v\in [m, m+dm])\delta(u > v)du dv\\
&=
\int_{v=m}^{m+dm}\int_{u=v}^\infty f^{(\beta)}_{C, T|X}(u,v) du dv\\
&=
\int_{v=m}^{m+dm}\int_{u=v}^\infty f_{C|X}(u)f^{(\beta)}_{T|X}(v) du dv\\
&=
\int_{v=m}^{m+dm}f^{(\beta)}_{T|X}(v)\int_{u=v}^\infty f_{C|X}(u) du dv \\
&=
\int_{v=m}^{m+dm}f^{(\beta)}_{T|X}(v)\int_{u=v}^\infty f_{C|X}(u) du dv \\
&=
\int_{v=m}^{m+dm}f^{(\beta)}_{T|X}(v)(1 - F_{C|X}(v)) dv.
\end{align*}

As noted in OH, if we then assume $dm$ gets small and that (think Riemann)
$$
\int_{v=m}^{m+dm} (1 - F_{C|X}(v))dv \approx (1 - F_{C|X}(m)) dm
$$
we may proceed with
$$
\int_{v=m}^{m+dm}f^{(\beta)}_{T|X}(v)(1 - F_{C|X}(v)) dv
\approx
(1 - F_{C|X}(v)) dm \cdot \int_{v=m}^{m+dm}f^{(\beta)}_{T|X}(v)
\approx
(1 - F_{C|X}(v)) dm \cdot h_\beta(m),
$$
based on how the hazard $h$ was originally defined. This is much more tractable, and the approximation follows from lecture slides `Survival-Analysis-4`. The same observation holds in reverse for the $\delta = 0$ case, i.e. 

\begin{align*}
P_\beta(M\in[m, m + dm], \delta = 0|X)
&=
P_\beta(C\in [m, m+dm], C < T | X) \\
&=
\int_{u=0}^\infty\int_{v=0}^\infty f^{(\beta)}_{C, T|X}(u, v)\delta(u\in [m, m+dm])\delta(u < v)dvdu\\
&=
\int_{u=m}^{m+dm}\int_{v=u}^\infty f^{(\beta)}_{C, T|X}(u,v) dv du\\
&=
\int_{u=m}^{m+dm}\int_{v=u}^\infty f_{C|X}(u)f^{(\beta)}_{T|X}(v) dv du\\
&=
\int_{u=m}^{m+dm} f_{C|X}(u)\int_{v=u}^\infty f^{(\beta)}_{T|X}(v) dv du \\
&=
\int_{u=m}^{m+dm} f_{C|X}(u)(1 - F^{(\beta)}_{T|X}(u))du \\
&=
\int_{u=m}^{m+dm} f_{C|X}(u)S_\beta(u) du.
\end{align*}

As before, if we *do not have conditional independence, we are stuck at the third line of the proof. We need conditional independence to go through!* As before, if we say that as dm gets small, we can go
$$
\int_{u=m}^{m+dm} S_\beta(u) du \approx S_\beta(m) dm
$$
and hence
$$
\int_{v=m}^{m+dm} f_{C|X}(u)S_\beta(u) du
\approx 
S_\beta(m) dm \int_{v=m}^{m+dm} f_{C|X}(u)du
\approx 
S_\beta(m) dm \cdot P_{{C|X}}(C\in[m, m+dm])
$$

and we can interpret $\int_{v=m}^{m+dm} f_{C|X}(u)du$ to be a probability if we need interpretability.


From class, our likelihood is
\begin{align*}
L(\beta, X_i, M_i)= \prod_{i=1}^n h_\beta(X_i, M_i)^{\delta_i} S_\beta(X_i, M_i) \\
&= \prod_{i=1}^n h_\beta(X_i, M_i)^{\delta_i} S_\beta(X_i, M_i)^{\delta_i}S_\beta(X_i, M_i)^{1 - \delta_i} \\
&= \prod_{i=1}^n f_\beta(X_i, M_i)^{\delta_i}S_\beta(X_i, M_i)^{1 - \delta_i},
\end{align*}

which for each term in the product, are effectively are our two cases as presented (albeit in integral-chunk form) above: the $f_\beta(X_i, M_i)^{\delta_i}$ term corresponds to the $\delta=1$ case at the start of the problem, while the $S_\beta(X_i, M_i)^{1 - \delta_i}$ corresponds to the $\delta=0$ case examined shortly thereafter. 


So in closing -- we showed above that when we have conditional independence, we can recover the right-censored likelihood we've been using. Of course, for all of this to work, we have to hold onto a few more assumptions, to wit:

* Censored patients would have had a similar survival profile as those who we got to see die, at all times in the data. 
* Proportioanl hazards
* No confounding over the study, i.e. there was no medicine introduced (or something equivalent) that later subjects more robust to survival.
