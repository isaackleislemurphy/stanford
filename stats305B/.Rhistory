rnorm(S),
rnorm(S),
rnorm(S)
)
glm(Y ~ V2 + V3 + V4, data=data.frame(cbind(X, Y)), family="binomial") -> base_fit
### spam data
spam_data = read.csv("/Users/IKleisle/Stanford/STATS305B/spam.csv")
X = spam_data %>%
dplyr::select(A.1:A.57) %>%
as.matrix() %>%
unname()
### mean and center scale
X = scale(X)
### fix outliers by truncation at 5 SD
X[X > 5] = 5; X[X < -5] = -5
X = cbind(1, X)
Y = as.integer(spam_data$spam == "spam")
summary((spam_data$A.56 - mean(spam_data$A.56)) / sd(spam_data$A.56))
#
#
glm(
Y ~ .,
data=data.frame(cbind(X, Y)), # use the scaled one
family=binomial()
) -> base_fit
solve_fisher_scoring(
X,
Y,
link=sigmoid,
deriv_link=function(x){sigmoid(x) * (1 - sigmoid(x))},
max_iter=50000,
tol=1e-5
)
coef(base_fit)
glm(
Y ~ . + -V1,
data=data.frame(cbind(X, Y)), # use the scaled one
family=binomial()
) -> base_fit
coef(base_fit)
#
#
glm(
Y ~ .,
data=data.frame(cbind(X, Y)), # use the scaled one
family=binomial()
) -> base_fit
solve_fisher_scoring(
X,
Y,
link=sigmoid,
deriv_link=function(x){sigmoid(x) * (1 - sigmoid(x))},
max_iter=50000,
tol=1e-5
)
glm(
Y ~ .,
data=data.frame(cbind(X[, 2:nrow(X)], Y)), # use the scaled one
family=binomial()
) -> base_fit
glm(
Y ~ .,
data=data.frame(cbind(X[, 2:ncol(X)], Y)), # use the scaled one
family=binomial()
) -> base_fit
coef(base_fit)
solve_fisher_scoring(
X,
Y,
link=function(x){pt(x, df=50)},
deriv_link=function(x){dt(x, df=50)},
max_iter=50000,
tol=1e-5
)
clip <- function(x, eps=1e-7){
x_ = x
x_[x_ == 0.] = eps
x_[x_ == 1.] = 1 - eps
x_
}
compute_deviance <- function(X, Y, link, beta, eps=1e-7){
#' Computes binary deviance for arbitrary link. See GLM slides 10.
#'
eta = as.numeric(
X %*% beta
)
dev = -2 * sum(
Y * log(
link(eta) / clip(1 - link(eta))
) +
log(1 - link(eta))
)
dev
}
compute_grad_deviance <- function(X, Y, link, deriv_link, beta, eps=1e-7){
#' Computes gradient of deviance
#'
eta = as.numeric(
X %*% beta
)
W_b = diag(
deriv_link(eta)^2 /
(
clip(link(eta) * (1 - link(eta)))
)
)
grad_dev = -2 * t(X) %*% W_b %*% (
(Y - link(eta)) / deriv_link(eta)
)
as.numeric(grad_dev)
}
compute_hessian_deviance <- function(X, Y, link, deriv_link, beta){
#' Computes gradient of deviance
#'
eta = as.numeric(
X %*% beta
)
W_b = diag(
deriv_link(eta)^2 /
(
clip(link(eta) * (1 - link(eta)))
)
)
result = 2 * t(X) %*% W_b %*% X
result
}
solve_fisher_scoring <- function(
X,
Y,
link,
# inverse_link,
deriv_link,
max_iter=50000,
tol=1e-5
){
# init beta and beta history log
beta = rep(0, dim(X)[2])
beta_log = list(beta)
### init bool + counter
is_converged = F
n_iter = 0
for (i in 2:(max_iter + 1)){
print(i)
### perform update
beta = beta -
solve(
compute_hessian_deviance(X, Y, link, deriv_link, beta)
) %*% compute_grad_deviance(
X, Y, link, deriv_link, beta
) %>% as.numeric()
### save
beta_log[[i]] = beta
### elementwise deltas
delta = beta - beta_log[[i - 1]]
### check convergence
if(sqrt(as.numeric(t(delta) %*% delta)) < tol){
is_converged = T
n_iter = i
break
}
# return(beta)
}
cat("###### Fisher Scoring Complete ######\n")
cat("Iterations: ", n_iter, "\n")
cat("Converged: ", is_converged, "\n")
cat("L2 Norm: ", sqrt(as.numeric(t(delta) %*% delta)), "\n")
cat("#####################################\n\n")
beta
}
solve_gradient_descent <- function(
X,
Y,
link,
# inverse_link,
deriv_link,
alpha=function(i){1e-3},
max_iter=50000,
tol=1e-5
){
# init beta and beta history log
beta = rep(0, dim(X)[2])
beta_log = list(beta)
### init bool + counter
is_converged = F
n_iter = 0
for (i in 2:(max_iter + 1)){
if (i %% 500 == 0){
print(i)
}
### perform update
delta_beta = alpha(i) * compute_grad_deviance(
X, Y, link, deriv_link, beta
) %>% as.numeric()
browser()
beta = beta - delta_beta
### save
beta_log[[i]] = beta
### elementwise deltas
delta = beta - beta_log[[i - 1]]
### check convergence
if(sqrt(as.numeric(t(delta) %*% delta)) < tol){
is_converged = T
n_iter = i
break
}
# return(beta)
}
cat("###### Gradient Descent Complete ######\n")
cat("Iterations: ", n_iter, "\n")
cat("Converged: ", is_converged, "\n")
cat("L2 Norm: ", sqrt(as.numeric(t(delta) %*% delta)), "\n")
cat("#######################################\n\n")
beta
}
clip <- function(x, eps=1e-7){
x_ = x
x_[x_ == 0.] = eps
x_[x_ == 1.] = 1 - eps
x_
}
compute_deviance <- function(X, Y, link, beta, eps=1e-7){
#' Computes binary deviance for arbitrary link. See GLM slides 10.
#'
eta = as.numeric(
X %*% beta
)
dev = -2 * sum(
Y * log(
link(eta) / clip(1 - link(eta))
) +
log(1 - link(eta))
)
dev
}
compute_grad_deviance <- function(X, Y, link, deriv_link, beta, eps=1e-7){
#' Computes gradient of deviance
#'
eta = as.numeric(
X %*% beta
)
W_b = diag(
deriv_link(eta)^2 /
(
clip(link(eta) * (1 - link(eta)))
)
)
grad_dev = -2 * t(X) %*% W_b %*% (
(Y - link(eta)) / deriv_link(eta)
)
as.numeric(grad_dev)
}
compute_hessian_deviance <- function(X, Y, link, deriv_link, beta){
#' Computes gradient of deviance
#'
eta = as.numeric(
X %*% beta
)
W_b = diag(
deriv_link(eta)^2 /
(
clip(link(eta) * (1 - link(eta)))
)
)
result = 2 * t(X) %*% W_b %*% X
result
}
solve_fisher_scoring <- function(
X,
Y,
link,
# inverse_link,
deriv_link,
max_iter=50000,
tol=1e-5
){
# init beta and beta history log
beta = rep(0, dim(X)[2])
beta_log = list(beta)
### init bool + counter
is_converged = F
n_iter = 0
for (i in 2:(max_iter + 1)){
print(i)
### perform update
beta = beta -
solve(
compute_hessian_deviance(X, Y, link, deriv_link, beta)
) %*% compute_grad_deviance(
X, Y, link, deriv_link, beta
) %>% as.numeric()
### save
beta_log[[i]] = beta
### elementwise deltas
delta = beta - beta_log[[i - 1]]
### check convergence
if(sqrt(as.numeric(t(delta) %*% delta)) < tol){
is_converged = T
n_iter = i
break
}
# return(beta)
}
cat("###### Fisher Scoring Complete ######\n")
cat("Iterations: ", n_iter, "\n")
cat("Converged: ", is_converged, "\n")
cat("L2 Norm: ", sqrt(as.numeric(t(delta) %*% delta)), "\n")
cat("#####################################\n\n")
beta
}
solve_gradient_descent <- function(
X,
Y,
link,
# inverse_link,
deriv_link,
alpha=function(i){1e-3},
max_iter=50000,
tol=1e-5
){
# init beta and beta history log
beta = rep(0, dim(X)[2])
beta_log = list(beta)
### init bool + counter
is_converged = F
n_iter = 0
for (i in 2:(max_iter + 1)){
if (i %% 500 == 0){
print(i)
print(max(abs(delta_beta)))
}
### perform update
delta_beta = alpha(i) * compute_grad_deviance(
X, Y, link, deriv_link, beta
) %>% as.numeric()
beta = beta - delta_beta
### save
beta_log[[i]] = beta
### elementwise deltas
delta = beta - beta_log[[i - 1]]
### check convergence
if(sqrt(as.numeric(t(delta) %*% delta)) < tol){
is_converged = T
n_iter = i
break
}
# return(beta)
}
cat("###### Gradient Descent Complete ######\n")
cat("Iterations: ", n_iter, "\n")
cat("Converged: ", is_converged, "\n")
cat("L2 Norm: ", sqrt(as.numeric(t(delta) %*% delta)), "\n")
cat("#######################################\n\n")
beta
}
solve_gradient_descent(
X,
Y,
link=sigmoid,
deriv_link=function(x){sigmoid(x) * (1 - sigmoid(x))},
alpha=function(i){1e-3},
max_iter=50000,
tol=1e-5
)
clip <- function(x, eps=1e-7){
x_ = x
x_[x_ == 0.] = eps
x_[x_ == 1.] = 1 - eps
x_
}
compute_deviance <- function(X, Y, link, beta, eps=1e-7){
#' Computes binary deviance for arbitrary link. See GLM slides 10.
#'
eta = as.numeric(
X %*% beta
)
dev = -2 * sum(
Y * log(
link(eta) / clip(1 - link(eta))
) +
log(1 - link(eta))
)
dev
}
compute_grad_deviance <- function(X, Y, link, deriv_link, beta, eps=1e-7){
#' Computes gradient of deviance
#'
eta = as.numeric(
X %*% beta
)
W_b = diag(
deriv_link(eta)^2 /
(
clip(link(eta) * (1 - link(eta)))
)
)
grad_dev = -2 * t(X) %*% W_b %*% (
(Y - link(eta)) / deriv_link(eta)
)
as.numeric(grad_dev)
}
compute_hessian_deviance <- function(X, Y, link, deriv_link, beta){
#' Computes gradient of deviance
#'
eta = as.numeric(
X %*% beta
)
W_b = diag(
deriv_link(eta)^2 /
(
clip(link(eta) * (1 - link(eta)))
)
)
result = 2 * t(X) %*% W_b %*% X
result
}
solve_fisher_scoring <- function(
X,
Y,
link,
# inverse_link,
deriv_link,
max_iter=50000,
tol=1e-5
){
# init beta and beta history log
beta = rep(0, dim(X)[2])
beta_log = list(beta)
### init bool + counter
is_converged = F
n_iter = 0
for (i in 2:(max_iter + 1)){
print(i)
### perform update
beta = beta -
solve(
compute_hessian_deviance(X, Y, link, deriv_link, beta)
) %*% compute_grad_deviance(
X, Y, link, deriv_link, beta
) %>% as.numeric()
### save
beta_log[[i]] = beta
### elementwise deltas
delta = beta - beta_log[[i - 1]]
### check convergence
if(sqrt(as.numeric(t(delta) %*% delta)) < tol){
is_converged = T
n_iter = i
break
}
# return(beta)
}
cat("###### Fisher Scoring Complete ######\n")
cat("Iterations: ", n_iter, "\n")
cat("Converged: ", is_converged, "\n")
cat("L2 Norm: ", sqrt(as.numeric(t(delta) %*% delta)), "\n")
cat("#####################################\n\n")
beta
}
solve_gradient_descent <- function(
X,
Y,
link,
# inverse_link,
deriv_link,
alpha=function(i){1e-3},
max_iter=50000,
tol=1e-5
){
# init beta and beta history log
beta = rep(0, dim(X)[2])
beta_log = list(beta)
### init bool + counter
is_converged = F
n_iter = 0
for (i in 2:(max_iter + 1)){
if (i %% 500 == 0){
print(i)
}
### perform update
delta_beta = alpha(i) * compute_grad_deviance(
X, Y, link, deriv_link, beta
) %>% as.numeric()
print(max(abs(delta_beta)))
beta = beta - delta_beta
### save
beta_log[[i]] = beta
### elementwise deltas
delta = beta - beta_log[[i - 1]]
### check convergence
if(sqrt(as.numeric(t(delta) %*% delta)) < tol){
is_converged = T
n_iter = i
break
}
# return(beta)
}
cat("###### Gradient Descent Complete ######\n")
cat("Iterations: ", n_iter, "\n")
cat("Converged: ", is_converged, "\n")
cat("L2 Norm: ", sqrt(as.numeric(t(delta) %*% delta)), "\n")
cat("#######################################\n\n")
beta
}
solve_gradient_descent(
X,
Y,
link=sigmoid,
deriv_link=function(x){sigmoid(x) * (1 - sigmoid(x))},
alpha=function(i){1e-3},
max_iter=50000,
tol=1e-5
)
